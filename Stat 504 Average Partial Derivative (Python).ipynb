{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b6ccc",
   "metadata": {},
   "source": [
    "# Simulate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88689fd3",
   "metadata": {},
   "source": [
    "Here we will draw $n=1,000$ from the model:\n",
    "$$\n",
    "X_i \\sim N(\\mu_x, \\sigma^2_{x})\\\\\n",
    "Y_i|X_i \\sim N(\\beta_0 + \\beta_1 X_i + \\beta_2X_i^2, \\sigma^2_{y.x} )\n",
    "$$\n",
    "With parameters defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bb53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# simulate data\n",
    "np.random.seed(10)\n",
    "n = 10000\n",
    "x = 0.5 + np.random.normal(0,1,n)\n",
    "y = 2 + x + 3*(x**2) + np.random.normal(0,1,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1cf23",
   "metadata": {},
   "source": [
    "The population average partial derivative is easy to compute analytically, and given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "APD = E\\left[\\frac{\\partial E[Y_i|X_i]}{\\partial X_i}\\right] &= \\beta_1 + 2\\times \\beta_2 \\times E[X_i]\\\\\n",
    "&= 1 + 2\\times (3)\\times (0.5)\\\\\n",
    "&= 4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1c200",
   "metadata": {},
   "source": [
    "# Estimation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1dddd",
   "metadata": {},
   "source": [
    "Using the plug-in principle, we can estimate the APD in two ways. First, if we know the analytical formula, like above, we can simply use the sample analogs:\n",
    "\n",
    "$$\n",
    "\\widehat{APD} = \\widehat{\\beta}_1 + 2\\times \\widehat{\\beta}_2 \\times E_n[X_i]\n",
    "$$\n",
    "\n",
    "Where $E_n[X_i] = \\frac{1}{n}\\sum_{i} X_i$ is the empirical mean. This leads to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d29668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021505239645409"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "data = {\"x\":x, \"y\":y}\n",
    "# fit regression\n",
    "ols=smf.ols(formula = 'y ~ x+np.power(x, 2)', data = data).fit()\n",
    "\n",
    "# plug-in estimate of APD using analytical formula \n",
    "APD1 = ols.params[1] + 2*ols.params[2]*np.mean(x)\n",
    "APD1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473666",
   "metadata": {},
   "source": [
    "We obtain 4.02, which is pretty close to the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bffd6",
   "metadata": {},
   "source": [
    "Now, recall the derivative can be defined as :\n",
    "    \n",
    "$$\n",
    "\\left. \\frac{\\partial E[Y_i \\mid X_i]}{\\partial X_i}\\right|_{X_i=x} = \\lim_{h\\to 0} \\frac{E[Y_i \\mid X_i= x +h] - E[Y_i \\mid X_i = x - h]}{2h}\n",
    "$$\n",
    "    \n",
    "This formula suggests an alternative to approximate the derivative numerically, by computing the above difference with a small enough $h$. \n",
    "\n",
    "Thus, appealing again to the plug-in principle, we can estimate the APD using a numerical approximation, and compute:\n",
    "    \n",
    "$$\n",
    "\\widehat{\\text{APD}} := E_{n}\\left[\\frac{\\widehat{E}[Y_i \\mid x_i +h] - \\widehat{E}[Y_i \\mid x_i - h]}{2h}\\right] = \\frac{1}{n} \\sum_{i}\\left[\\frac{\\widehat{E}[Y_i \\mid x_i +h] - \\widehat{E}[Y_i \\mid x_i - h]}{2h}\\right]\n",
    "$$\n",
    "This leads to the following estimate, using $h=0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c6dab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021505239646939"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plug-in estimate of APD using numerical derivative\n",
    "h = 0.0001\n",
    "yp = ols.predict({\"x\":x + h})\n",
    "ym = ols.predict({\"x\":x - h})\n",
    "dx = (yp - ym)/(2*h)\n",
    "APD2 = np.mean(dx)\n",
    "APD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300e435",
   "metadata": {},
   "source": [
    "Which is essentially numerically identical to the previous estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b86655",
   "metadata": {},
   "source": [
    "# Inference using the nonparametric bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b94f2",
   "metadata": {},
   "source": [
    "One way to perform inference on the APD is using the nonparametric bootstrap. Here we will bootstrap both estimators at the same time. \n",
    "Note you need to **bootstrap the whole procedure**. That is:\n",
    "* Sample with replacement $n$ rows from the data;\n",
    "* Refit the OLS model;\n",
    "* Recompute the APD using the refited model and resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7daf2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bootstrap function\n",
    "def boot_fun():\n",
    "    # sample rows\n",
    "    idx=np.random.choice(np.arange(n), size=n, replace=True)\n",
    "    \n",
    "    # fit OLS to boostrapped cases\n",
    "    x_boot   = x[idx]\n",
    "    y_boot   = y[idx]\n",
    "    data = {\"x\":x_boot, \"y\":y_boot}\n",
    "    ols=smf.ols(formula = 'y ~ x+np.power(x, 2)', data = data).fit()\n",
    "\n",
    "    # recompute plug-in estimate using analytical formula \n",
    "    APD1 = ols.params[1] + 2*ols.params[2]*np.mean(x_boot)\n",
    "    \n",
    "    # recompute plug-in estimate using numerical derivative \n",
    "    h = 0.0001\n",
    "    yp = ols.predict({\"x\":x_boot + h})\n",
    "    ym = ols.predict({\"x\":x_boot - h})\n",
    "    dx = (yp - ym)/(2*h)\n",
    "    APD2 = np.mean(dx)\n",
    "    APD2\n",
    "    \n",
    "    # return both\n",
    "    return(APD1,APD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d00d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now replicate the function 10,000 times\n",
    "# this gives us 10,000 bootstrap samples\n",
    "boot_samples=np.zeros((10000,2))\n",
    "for i in range(10000):\n",
    "    result=boot_fun()\n",
    "    boot_samples[i,0]=result[0]\n",
    "    boot_samples[i,1]=result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f88267",
   "metadata": {},
   "source": [
    "Now we can construct confidence intervals. Here we will use two methods: (1) the percentile method; and, (2) the normal approximation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a996d69",
   "metadata": {},
   "source": [
    "1. Percentile method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cdab9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile CI for APD1: [3.90060135 4.13968833]\n",
      "Percentile CI for APD2: [3.90060135 4.13968833]\n"
     ]
    }
   ],
   "source": [
    "# set significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Percentile CI\n",
    "\n",
    "## CI for APD1\n",
    "print(\"Percentile CI for APD1:\",np.quantile(boot_samples[:,0],[alpha/2,1-alpha/2]))\n",
    "\n",
    "\n",
    "## CI for APD2\n",
    "print(\"Percentile CI for APD2:\",np.quantile(boot_samples[:,1], (alpha/2, 1-alpha/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313583f5",
   "metadata": {},
   "source": [
    "2. Normal approximation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a60468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal approximation CI for APD1: 3.902465111316791 4.140545367975556\n",
      "Normal approximation CI for APD2: 3.902465111318246 4.1405453679756326\n"
     ]
    }
   ],
   "source": [
    "# Normal approximation CI\n",
    "\n",
    "## critical threshold\n",
    "from scipy.stats import norm\n",
    "z = norm.ppf(1-alpha/2)\n",
    "\n",
    "\n",
    "## compute bootstrap standard errors\n",
    "sd_boot_1 = np.std(boot_samples[:,0])\n",
    "sd_boot_2 = np.std(boot_samples[:,1])\n",
    "\n",
    "## CI for APD1\n",
    "print(\"Normal approximation CI for APD1:\",APD1-z*sd_boot_1,APD2+z*sd_boot_1)\n",
    "\n",
    "## CI for APD2\n",
    "print(\"Normal approximation CI for APD2:\",APD2-z*sd_boot_2,APD2+z*sd_boot_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd0d3ca1c00f977588ad6d8d33ad3ddd9dc06389ddd86a87423e5e5ddfb9f1cf524"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
